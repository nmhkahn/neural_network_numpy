{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "import nn.layers as layers\n",
    "import nn.optim as optim\n",
    "import nn.net as net\n",
    "from nn.solver import Solver\n",
    "from utils import *\n",
    "from dataset import get_CIFAR10_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-conneceted (Linear) layer\n",
    "`nn/layers.py` 파일 내부에 있는 `Linear` 클래스의 `forward`와 `backward` 메소드를 주석에 명시된 요구 조건에 따라 작성하고, 아래 코드를 실행시켜 구현한 코드를 테스트한다.<br>\n",
    "**NOTE**: 모든 테스트 결과는 *1e-06* 이하의 오차만 허용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Linear.forward()\n",
    "num_input, in_dims, out_dims = 3, 5, 3\n",
    "\n",
    "linear = layers.Linear(in_dims, out_dims, init_mode=\"linear\")\n",
    "x = np.linspace(-0.1, 0.5, num=num_input*in_dims).reshape(num_input, in_dims)\n",
    "\n",
    "out = linear.forward(x)\n",
    "correct_out = np.array([[ 0.01938776,  0.01836735,  0.01734694],\n",
    "                        [ 0.00408163,  0.01836735,  0.03265306],\n",
    "                        [-0.01122449,  0.01836735,  0.04795918],])\n",
    "print(\"Testing linear - forward function:\")\n",
    "print(\"error:\", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Linear.backward()\n",
    "num_input, in_dims, out_dims = 10, 6, 5\n",
    "\n",
    "linear = layers.Linear(in_dims, out_dims, init_mode=\"normal\")\n",
    "x = np.linspace(-0.1, 0.5, num=num_input*in_dims).reshape(num_input, in_dims)\n",
    "dout = np.random.randn(num_input, out_dims)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: linear.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: linear.forward(x), linear.params[\"w\"], dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: linear.forward(x), linear.params[\"b\"], dout)\n",
    "\n",
    "out = linear.forward(x)\n",
    "linear.backward(dout)\n",
    "\n",
    "dx, dw, db = linear.grads[\"x\"], linear.grads[\"w\"], linear.grads[\"b\"]\n",
    "\n",
    "print(\"Testing linear - backward function:\")\n",
    "print(\"dx error:\", rel_error(dx, dx_num))\n",
    "print(\"dw error:\", rel_error(dw, dw_num))\n",
    "print(\"db error:\", rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "`nn/layers.py` 파일 내부에 있는 `ReLU`, `Sigmoid`, `Tanh` 클래스의 `forward` 및 `backward` 메소드를 주석에 명시된 요구 조건에 따라 작성하고, 아래 코드를 실행시켜 구현한 코드를 테스트한다.<br>\n",
    "**NOTE**: 모든 테스트 결과는 *1e-06* 이하의 오차만 허용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ReLU.forward()\n",
    "relu = layers.ReLU()\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out = relu.forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "print(\"Testing ReLU - forward function:\")\n",
    "print(\"error:\", rel_error(out, correct_out))\n",
    "\n",
    "# Test the ReLU.backward()\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu.forward(x), x, dout)\n",
    "\n",
    "relu.forward(x)\n",
    "dx = relu.backward(dout)\n",
    "\n",
    "print(\"\\nTesting ReLU - backward function:\")\n",
    "print(\"dx error:\", rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Sigmoid.forward()\n",
    "sigmoid = layers.Sigmoid()\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out = sigmoid.forward(x)\n",
    "correct_out = np.array([[0.37754067, 0.39913012, 0.42111892, 0.44342513],\n",
    "                        [0.46596182, 0.48863832, 0.51136168, 0.53403818],\n",
    "                        [0.55657487, 0.57888108, 0.60086988, 0.62245933],])\n",
    "\n",
    "print(\"Testing Sigmoid - forward function:\")\n",
    "print(\"error:\", rel_error(out, correct_out))\n",
    "\n",
    "# Test the Sigmoid.backward()\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: sigmoid.forward(x), x, dout)\n",
    "\n",
    "sigmoid.forward(x)\n",
    "dx = sigmoid.backward(dout)\n",
    "\n",
    "print(\"\\nTesting Sigmoid - backward function:\")\n",
    "print(\"dx error:\", rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Tanh.forward()\n",
    "tanh = layers.Tanh()\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out = tanh.forward(x)\n",
    "correct_out = np.array([[-0.46211716, -0.38770051, -0.30786199, -0.22343882],\n",
    "                        [-0.13552465, -0.04542327,  0.04542327,  0.13552465],\n",
    "                        [ 0.22343882,  0.30786199,  0.38770051,  0.46211716],])\n",
    "\n",
    "print(\"Testing Tanh - forward function:\")\n",
    "print(\"error:\", rel_error(out, correct_out))\n",
    "\n",
    "# Test the Tanh.backward()\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: tanh.forward(x), x, dout)\n",
    "\n",
    "tanh.forward(x)\n",
    "dx = tanh.backward(dout)\n",
    "\n",
    "print(\"\\nTesting Tanh - backward function:\")\n",
    "print(\"dx error:\", rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layer: Softmax-CrossEntropy\n",
    "`nn/layers.py` 파일 내부에 있는 `SoftmaxCELoss` 클래스의 `forward` 메소드를 주석에 명시된 요구 조건에 따라 작성하고, 아래 코드를 실행시켜 구현한 코드를 테스트한다.<br>\n",
    "주의: 구현의 편의성을 위해 별도의 `backward` 메소드 없이 `forward` 함수 내부에서 loss와 gradient를 계산하도록 되어있다.<br>\n",
    "**NOTE**: 모든 테스트 결과는 *1e-06* 이하의 오차만 허용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "ce = layers.SoftmaxCELoss()\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: ce.forward(x, y)[0], x, verbose=False)\n",
    "loss, dx = ce.forward(x, y)\n",
    "\n",
    "print(\"Testing softmax loss:\")\n",
    "print(\"loss:\", loss, \" (must be around 2.3)\")\n",
    "print(\"dx error:\", rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer Network\n",
    "이전까지 구현한 모듈들을 조합하여 레이어 2개짜리 네트워크를 작성한다. `nn/net.py` 파일에 있는 `TwoLayerNet` 클래스의 생성자 및 `loss` 함수를 구현한 뒤, 아래 코드를 실행하여 테스트 한다.<br>\n",
    "**NOTE**: 모든 테스트 결과는 *1e-06* 이하의 오차만 허용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "y = np.asarray([0, 5, 1])\n",
    "\n",
    "mynet = net.TwoLayerNet(D, H, C)\n",
    "mynet.modules[\"linear1\"].params[\"w\"] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "mynet.modules[\"linear1\"].params[\"b\"] = np.linspace(-0.1, 0.9, num=H)\n",
    "mynet.modules[\"linear2\"].params[\"w\"] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "mynet.modules[\"linear2\"].params[\"b\"] = np.linspace(-0.9, 0.1, num=C)\n",
    "\n",
    "scores = mynet.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "correct_loss = 3.4702243556\n",
    "\n",
    "print(\"Testing TwoLayerNet:\")\n",
    "print(\"score error:\", np.abs(scores - correct_scores).sum())\n",
    "\n",
    "loss = mynet.loss(X, y)\n",
    "print(\"loss error:\", np.abs(loss - correct_loss))\n",
    "\n",
    "loss = mynet.loss(X, y)\n",
    "f = lambda _: mynet.loss(X, y)\n",
    "grad_w2 = eval_numerical_gradient(f, mynet.modules[\"linear2\"].params[\"w\"], verbose=False)\n",
    "grad_b2 = eval_numerical_gradient(f, mynet.modules[\"linear2\"].params[\"b\"], verbose=False)\n",
    "grad_w1 = eval_numerical_gradient(f, mynet.modules[\"linear1\"].params[\"w\"], verbose=False)\n",
    "grad_b1 = eval_numerical_gradient(f, mynet.modules[\"linear1\"].params[\"b\"], verbose=False)\n",
    "\n",
    "print()\n",
    "print(\"grad_w2 error:\", rel_error(grad_w2, mynet.modules[\"linear2\"].grads[\"w\"]))\n",
    "print(\"grad_b2 error:\", rel_error(grad_b2, mynet.modules[\"linear2\"].grads[\"b\"]))\n",
    "print(\"grad_w1 error:\", rel_error(grad_w1, mynet.modules[\"linear1\"].grads[\"w\"]))\n",
    "print(\"grad_b1 error:\", rel_error(grad_b1, mynet.modules[\"linear1\"].grads[\"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver and train the network\n",
    "위에서 작성한 2-레이어 네트워크를 학습한다. 네트워크의 학습을 CIFAR-10 데이터를 사용하여 (`dataset.py` 참조), `nn.Solver` 파일의 `Solver` 클래스에 의해 학습이 진행된다. 대부분의 코드가 미리 작성되었기 때문에  `nn.Solver` 파일의 클래스의 API를 참조하여 아래 **TODO** 부분을 작성한다. **TODO** 부분은 네트워크 하이퍼파라미터 세팅에 관한 코드가 들어가야 하며, validation set에 대해 **50%** 이상의 성능을 보여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net.TwoLayerNet(init_mode=\"normal\", init_scale=0.001)\n",
    "solver = None\n",
    "\n",
    "######################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves   #\n",
    "# at least **50%** accuracy on the validation set.                   #\n",
    "######################################################################\n",
    "\n",
    "######################################################################\n",
    "#                          END OF YOUR CODE                          #\n",
    "######################################################################\n",
    "solver.train()\n",
    "\n",
    "# plot results\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"Training loss\")\n",
    "plt.plot(solver.loss_history, \"-\")\n",
    "plt.xlabel(\"Steps\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(solver.train_acc_history, \"-o\", label=\"train\")\n",
    "plt.plot(solver.val_acc_history, \"-o\", label=\"val\")\n",
    "plt.plot([0.5] * len(solver.val_acc_history), \"k--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer network\n",
    "이제 `nn/net.py`의 `FCNet` 클래스의 빈칸을 작성하여, 레이어가 여러 개인 네트워크를 디자인한다. 이 때, 임의의 레이어 개수를 입력으로 받도록 for loop를 사용해서 네트워크를 구성하고 forward/backward 연산을 수행해야만 한다. 그 후, 네트워크를 테스트하고 (numerical 이슈에 의해 상대 오차가 비교적 클 가능성이 있으므로, 테스트 코드를 반복 실행하여 바뀌는 오차를 확인한다.) `TwoLayerNet` 과 동일한 하이퍼파라미터를 사용하여 네트워크를 학습한다.<br>\n",
    "**주의**: 성능이 낮게 나오는 것이 정상이므로 여기서 별도의 하이퍼파라미터 튜닝을 진행하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "model = net.FCNet(\n",
    "    input_dim=D, num_classes=C,\n",
    "    hidden_dims=[H1, H2],\n",
    "    init_scale=5e-2, init_mode=\"normal\"\n",
    ")\n",
    "\n",
    "loss = model.loss(X, y)\n",
    "print(\"Initial loss:\", loss)\n",
    "\n",
    "for module_name in model.modules.keys():\n",
    "    if not model.modules[module_name].params:\n",
    "        continue\n",
    "        \n",
    "    w = model.modules[module_name].params[\"w\"]\n",
    "    b = model.modules[module_name].params[\"b\"]\n",
    "    dw = model.modules[module_name].grads[\"w\"]\n",
    "    db = model.modules[module_name].grads[\"b\"]\n",
    "    \n",
    "    f = lambda _: model.loss(X, y)\n",
    "    grad_w = eval_numerical_gradient(f, w, verbose=False, h=1e-5)\n",
    "    grad_b = eval_numerical_gradient(f, b, verbose=False, h=1e-5)\n",
    "\n",
    "    print(\"{}_w error: {:e}\".format(module_name, rel_error(dw, grad_w)))\n",
    "    print(\"{}_b error: {:e}\".format(module_name, rel_error(db, grad_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net.FCNet(\n",
    "    hidden_dims=[100, 100],\n",
    "    init_scale=0.001, init_mode=\"normal\"\n",
    ")\n",
    "solver = None\n",
    "\n",
    "######################################################################\n",
    "# TODO: Use the same solver settings that used when training         #\n",
    "# TwoLayerNet model                                                  #\n",
    "######################################################################\n",
    "\n",
    "######################################################################\n",
    "#                          END OF YOUR CODE                          #\n",
    "######################################################################\n",
    "solver.train()\n",
    "\n",
    "# plot results\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"Training loss\")\n",
    "plt.plot(solver.loss_history, \"-\")\n",
    "plt.xlabel(\"Steps\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(solver.train_acc_history, \"-o\", label=\"train\")\n",
    "plt.plot(solver.val_acc_history, \"-o\", label=\"val\")\n",
    "plt.plot([0.5] * len(solver.val_acc_history), \"k--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better initialization\n",
    "지금까지는 네트워크의 weight를 std=0.001인 가우시안 분포를 사용하여 초기화 하였다 (`nn/init.py`의 `_normal_init` 함수 참조). 하지만 이 초기화 방법은 위와 같이 레이어를 깊게 쌓은 경우 성능을 올리기가 매우 어렵다는 단점이 있다. 이를 해결하고자 Xavier [1], He [2] 초기화 방법이 제안되었다.<br>\n",
    "아래 코드를 실행시켜보고 문제에 대해 답을 서술해보자.\n",
    "\n",
    "[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" ICAIS 2010.<br>\n",
    "[2] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" ICCV 2015.\n",
    "\n",
    "## Question:\n",
    "1. std=0.001인 가우시안 분포로 네트워크의 weight를 초기화한 모델의 학습이 되지 않는 이유가 무엇일까?\n",
    "2. He 초기화 방법은 가우시안 분포 초기화 어떤 장점을 가지고 있을까?\n",
    "\n",
    "## Answer:\n",
    "**답변을 여기에 작성한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net.FCNet(\n",
    "    hidden_dims=[100, 100],\n",
    "    init_mode=\"he\" # or use xavier\n",
    ")\n",
    "solver = None\n",
    "\n",
    "######################################################################\n",
    "# TODO: Use the same solver settings that used when training         #\n",
    "# TwoLayerNet model                                                  #\n",
    "######################################################################\n",
    "\n",
    "######################################################################\n",
    "#                          END OF YOUR CODE                          #\n",
    "######################################################################\n",
    "solver.train()\n",
    "\n",
    "# plot results\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(\"Training loss\")\n",
    "plt.plot(solver.loss_history, \"-\")\n",
    "plt.xlabel(\"Steps\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(solver.train_acc_history, \"-o\", label=\"train\")\n",
    "plt.plot(solver.val_acc_history, \"-o\", label=\"val\")\n",
    "plt.plot([0.5] * len(solver.val_acc_history), \"k--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
